{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessingパッケージについて"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**概要**<br>\n",
    "日本語の自然言語処理における前処理の関数をまとめたモジュールです<br><br>\n",
    "**一般的な前処理の流れ**<br>\n",
    "（np.nanの文章、もしくは変な読み込まれ方をした文を修正）<br>\n",
    "ストップワードの除去<br>\n",
    "数字の正規化<br>\n",
    "（表現の正規化）<br>\n",
    "分かち書き<br>\n",
    "コーパス作成<br>\n",
    "頻度を元に語句を除去<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**構成**<br>\n",
    "preprocess.py : 以下のモジュールを使った前処理クラスを提供<br>\n",
    "remove_stopwords.py : ストップワード（分析する上で使わない言葉）の削除<br>\n",
    "stopwords_config.py : ストップワードのパターンを保存しておく<br>\n",
    "normalize_number.py : 数字を正規化<br>\n",
    "mecab_handlers.py : mecabモジュールの読み込み・分かち書き関数<br>\n",
    "make_corpus.py : gensimを使って辞書やコーパスを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"朝９時に目覚めると、1111円札が99枚見つかって、１６時まで寝てしまいました。\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## remove_stopwords.py, stopwords_config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ストップワードのパターン\n",
    "from stopwords_config import stop_words_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['。', '、', '・', ',', '*', '＊', '（', '）', '(', ')', '!', '！', '?', '？', '[', ']', '「', '」', '『', '』', '【', '】', '■', '□', '●', '○', '◆', '◇', '★', '☆', '~', '〜', '\"', '”', '\\n', '/']\n"
     ]
    }
   ],
   "source": [
    "print(stop_words_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ストップワードを削除\n",
    "from remove_stopwords import remove_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "朝９時に目覚めると1111円札が99枚見つかって１６時まで寝てしまいました\n"
     ]
    }
   ],
   "source": [
    "removed = remove_stopwords(doc=doc, stopwords=stop_words_1)\n",
    "print(removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalize_number.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数字を全て同じ値にする\n",
    "from normalize_number import normalize_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "朝0時に目覚めると、0円札が0枚見つかって、0時まで寝てしまいました。\n"
     ]
    }
   ],
   "source": [
    "normalized = normalize_number(doc=doc, replace=\"0\")\n",
    "print(normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特定のパターンで出現した数字を同じ値に変換\n",
    "from normalize_number import normalize_number_specified_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "朝hoge時に目覚めると、1111円札が99枚見つかって、１６時まで寝てしまいました。\n"
     ]
    }
   ],
   "source": [
    "normalized = normalize_number_specified_pattern(doc=doc, patterns={\"prefix\":[\"朝\"], \"suffix\":[]}, replace=\"hoge\")\n",
    "print(normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "朝hoge時に目覚めると、1111円札がhoge枚見つかって、hoge時まで寝てしまいました。\n"
     ]
    }
   ],
   "source": [
    "normalized = normalize_number_specified_pattern(doc=doc, patterns={\"prefix\":[\"朝\"], \"suffix\":[\"時\", \"枚\"]}, replace=\"hoge\")\n",
    "print(normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mecab_handlers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分かち書き\n",
    "from mecab_handlers import doc_to_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['朝', '９', '時', 'に', '目覚める', 'と', '、', '1111', '円', '札', 'が', '99', '枚', '見つかっ', 'て', '、', '１', '６', '時', 'まで', '寝', 'て', 'しまい', 'まし', 'た', '。']\n"
     ]
    }
   ],
   "source": [
    "split = doc_to_words(doc=doc)\n",
    "print(split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['朝', '９', '時', '1111', '円', '札', '99', '枚', '１', '６', '時']\n",
      "['朝', '９', '時', '目覚める', '1111', '円', '札', '99', '枚', '見つかっ', '１', '６', '時', '寝', 'しまい']\n"
     ]
    }
   ],
   "source": [
    "split_only_noun = doc_to_words(doc, parts=[\"名詞\"])\n",
    "split_noun_and_verb = doc_to_words(doc, parts=[\"名詞\", \"動詞\"])\n",
    "print(split_only_noun)\n",
    "print(split_noun_and_verb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make_corpus.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensimで辞書を作成\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from preprocessing.make_corpus import make_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['今日', '天気'], ['東京', '渋谷', 'カフェ'], ['昆虫', '博士', '茂木', '健一郎']]\n"
     ]
    }
   ],
   "source": [
    "docs = [\"今日はいい天気\", \"東京の渋谷のカフェ\", \"昆虫博士と茂木健一郎\"]\n",
    "words_list = [doc_to_words(doc, parts=[\"名詞\"]) for doc in docs]\n",
    "print(words_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(9 unique tokens: ['今日', '天気', 'カフェ', '東京', '渋谷']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = make_dictionary(words_list)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '今日')\n",
      "(1, '天気')\n",
      "(2, 'カフェ')\n",
      "(3, '東京')\n",
      "(4, '渋谷')\n",
      "(5, '健一郎')\n",
      "(6, '博士')\n",
      "(7, '昆虫')\n",
      "(8, '茂木')\n"
     ]
    }
   ],
   "source": [
    "for i in dictionary.items():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'健一郎'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.id2token[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.token2id[\"カフェ\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make_corpus.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 事前にgensimで作った辞書を元に、コーパスを作成\n",
    "sys.path.append(\"../\")\n",
    "from preprocessing.make_corpus import  make_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1)], [(2, 1), (3, 1), (4, 1)], [(5, 1), (6, 1), (7, 1), (8, 1)]]\n"
     ]
    }
   ],
   "source": [
    "corpus = make_corpus(dictionary=dictionary, words_list=words_list)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0番目の文章で、単語「今日」が1回出現しました\n",
      "0番目の文章で、単語「天気」が1回出現しました\n",
      "1番目の文章で、単語「カフェ」が1回出現しました\n",
      "1番目の文章で、単語「東京」が1回出現しました\n",
      "1番目の文章で、単語「渋谷」が1回出現しました\n",
      "2番目の文章で、単語「健一郎」が1回出現しました\n",
      "2番目の文章で、単語「博士」が1回出現しました\n",
      "2番目の文章で、単語「昆虫」が1回出現しました\n",
      "2番目の文章で、単語「茂木」が1回出現しました\n"
     ]
    }
   ],
   "source": [
    "for i, l in enumerate(corpus):\n",
    "    for tup in l:\n",
    "        word_id = tup[0]\n",
    "        word = dictionary.id2token[word_id]\n",
    "        n = tup[1]\n",
    "        print(\"{}番目の文章で、単語「{}」が{}回出現しました\".format(i, word, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['今日はいい天気', '東京の渋谷のカフェ', '昆虫博士と茂木健一郎']\n"
     ]
    }
   ],
   "source": [
    "print(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "process = Preprocessing(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
